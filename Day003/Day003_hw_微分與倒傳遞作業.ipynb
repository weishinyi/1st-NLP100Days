{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxtArZ7u16sr"
   },
   "source": [
    "### 作業目標: 使用Pytorch進行微分與倒傳遞\n",
    "這份作業我們會實作微分與倒傳遞以及使用Pytorch的Autograd。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_BwC7sg16ss"
   },
   "source": [
    "### 使用Pytorch實作微分與倒傳遞\n",
    "\n",
    "這裡我們很簡單的實作兩層的神經網路進行回歸問題，其中loss function為L2 loss\n",
    "\n",
    "$$\n",
    "L2\\_loss = (y_{pred}-y)^2\n",
    "$$\n",
    "\n",
    "兩層經網路如下所示\n",
    "$$\n",
    "y_{pred} = ReLU(XW_1)W_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ocsA8ch-16st"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o2v8hkG616sz",
    "outputId": "0b737d18-59c2-4bb7-f541-e0ca6ab51a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0   loss: 169438688.0\n",
      "step: 1   loss: 7283528.5\n",
      "step: 2   loss: 2385691.5\n",
      "step: 3   loss: 1025069.6875\n",
      "step: 4   loss: 503857.65625\n",
      "step: 5   loss: 281201.59375\n",
      "step: 6   loss: 180641.875\n",
      "step: 7   loss: 133140.0625\n",
      "step: 8   loss: 109414.109375\n",
      "step: 9   loss: 96563.8828125\n",
      "step: 10   loss: 88776.390625\n",
      "step: 11   loss: 83405.3359375\n",
      "step: 12   loss: 79240.2734375\n",
      "step: 13   loss: 75722.40625\n",
      "step: 14   loss: 72581.8203125\n",
      "step: 15   loss: 69694.78125\n",
      "step: 16   loss: 66998.4375\n",
      "step: 17   loss: 64459.4921875\n",
      "step: 18   loss: 62057.34375\n",
      "step: 19   loss: 59778.7421875\n",
      "step: 20   loss: 57613.35546875\n",
      "step: 21   loss: 55553.7265625\n",
      "step: 22   loss: 53593.515625\n",
      "step: 23   loss: 51726.6484375\n",
      "step: 24   loss: 49947.49609375\n",
      "step: 25   loss: 48251.23828125\n",
      "step: 26   loss: 46633.66796875\n",
      "step: 27   loss: 45090.2734375\n",
      "step: 28   loss: 43615.72265625\n",
      "step: 29   loss: 42206.4921875\n",
      "step: 30   loss: 40858.8125\n",
      "step: 31   loss: 39569.96875\n",
      "step: 32   loss: 38337.59375\n",
      "step: 33   loss: 37157.0546875\n",
      "step: 34   loss: 36025.8125\n",
      "step: 35   loss: 34941.2890625\n",
      "step: 36   loss: 33901.3359375\n",
      "step: 37   loss: 32904.48828125\n",
      "step: 38   loss: 31947.994140625\n",
      "step: 39   loss: 31029.56640625\n",
      "step: 40   loss: 30147.25\n",
      "step: 41   loss: 29299.2734375\n",
      "step: 42   loss: 28484.31640625\n",
      "step: 43   loss: 27700.5234375\n",
      "step: 44   loss: 26946.15234375\n",
      "step: 45   loss: 26219.875\n",
      "step: 46   loss: 25520.482421875\n",
      "step: 47   loss: 24846.626953125\n",
      "step: 48   loss: 24197.314453125\n",
      "step: 49   loss: 23571.501953125\n",
      "step: 50   loss: 22968.400390625\n",
      "step: 51   loss: 22386.794921875\n",
      "step: 52   loss: 21825.626953125\n",
      "step: 53   loss: 21284.154296875\n",
      "step: 54   loss: 20761.25\n",
      "step: 55   loss: 20256.19140625\n",
      "step: 56   loss: 19768.333984375\n",
      "step: 57   loss: 19296.955078125\n",
      "step: 58   loss: 18841.30859375\n",
      "step: 59   loss: 18400.658203125\n",
      "step: 60   loss: 17974.671875\n",
      "step: 61   loss: 17562.4375\n",
      "step: 62   loss: 17163.49609375\n",
      "step: 63   loss: 16777.302734375\n",
      "step: 64   loss: 16403.376953125\n",
      "step: 65   loss: 16041.1328125\n",
      "step: 66   loss: 15689.99609375\n",
      "step: 67   loss: 15349.662109375\n",
      "step: 68   loss: 15019.6875\n",
      "step: 69   loss: 14699.740234375\n",
      "step: 70   loss: 14389.46875\n",
      "step: 71   loss: 14088.5625\n",
      "step: 72   loss: 13796.5126953125\n",
      "step: 73   loss: 13513.0546875\n",
      "step: 74   loss: 13237.9892578125\n",
      "step: 75   loss: 12970.890625\n",
      "step: 76   loss: 12711.4599609375\n",
      "step: 77   loss: 12459.46484375\n",
      "step: 78   loss: 12214.5859375\n",
      "step: 79   loss: 11976.638671875\n",
      "step: 80   loss: 11745.2080078125\n",
      "step: 81   loss: 11520.357421875\n",
      "step: 82   loss: 11301.73046875\n",
      "step: 83   loss: 11089.0234375\n",
      "step: 84   loss: 10882.099609375\n",
      "step: 85   loss: 10680.689453125\n",
      "step: 86   loss: 10484.7021484375\n",
      "step: 87   loss: 10293.8681640625\n",
      "step: 88   loss: 10108.24609375\n",
      "step: 89   loss: 9927.44921875\n",
      "step: 90   loss: 9751.279296875\n",
      "step: 91   loss: 9579.58984375\n",
      "step: 92   loss: 9412.224609375\n",
      "step: 93   loss: 9249.203125\n",
      "step: 94   loss: 9090.326171875\n",
      "step: 95   loss: 8935.3564453125\n",
      "step: 96   loss: 8784.107421875\n",
      "step: 97   loss: 8636.546875\n",
      "step: 98   loss: 8492.5537109375\n",
      "step: 99   loss: 8352.0966796875\n",
      "step: 100   loss: 8215.0224609375\n",
      "step: 101   loss: 8081.181640625\n",
      "step: 102   loss: 7950.5126953125\n",
      "step: 103   loss: 7822.90771484375\n",
      "step: 104   loss: 7698.34814453125\n",
      "step: 105   loss: 7576.6875\n",
      "step: 106   loss: 7457.84619140625\n",
      "step: 107   loss: 7341.73486328125\n",
      "step: 108   loss: 7228.28955078125\n",
      "step: 109   loss: 7117.4033203125\n",
      "step: 110   loss: 7008.9912109375\n",
      "step: 111   loss: 6903.0087890625\n",
      "step: 112   loss: 6799.365234375\n",
      "step: 113   loss: 6698.0830078125\n",
      "step: 114   loss: 6599.08349609375\n",
      "step: 115   loss: 6502.27734375\n",
      "step: 116   loss: 6407.53369140625\n",
      "step: 117   loss: 6314.849609375\n",
      "step: 118   loss: 6224.119140625\n",
      "step: 119   loss: 6135.31982421875\n",
      "step: 120   loss: 6048.37646484375\n",
      "step: 121   loss: 5963.27294921875\n",
      "step: 122   loss: 5879.94140625\n",
      "step: 123   loss: 5798.3271484375\n",
      "step: 124   loss: 5718.42578125\n",
      "step: 125   loss: 5640.14501953125\n",
      "step: 126   loss: 5563.486328125\n",
      "step: 127   loss: 5488.4658203125\n",
      "step: 128   loss: 5414.921875\n",
      "step: 129   loss: 5342.8671875\n",
      "step: 130   loss: 5272.24658203125\n",
      "step: 131   loss: 5203.0009765625\n",
      "step: 132   loss: 5135.12060546875\n",
      "step: 133   loss: 5068.59375\n",
      "step: 134   loss: 5003.40966796875\n",
      "step: 135   loss: 4939.48046875\n",
      "step: 136   loss: 4876.77197265625\n",
      "step: 137   loss: 4815.2529296875\n",
      "step: 138   loss: 4754.8974609375\n",
      "step: 139   loss: 4695.67529296875\n",
      "step: 140   loss: 4637.556640625\n",
      "step: 141   loss: 4580.509765625\n",
      "step: 142   loss: 4524.544921875\n",
      "step: 143   loss: 4469.64892578125\n",
      "step: 144   loss: 4415.76220703125\n",
      "step: 145   loss: 4362.857421875\n",
      "step: 146   loss: 4310.892578125\n",
      "step: 147   loss: 4259.8671875\n",
      "step: 148   loss: 4209.751953125\n",
      "step: 149   loss: 4160.517578125\n",
      "step: 150   loss: 4112.1630859375\n",
      "step: 151   loss: 4064.668212890625\n",
      "step: 152   loss: 4018.0009765625\n",
      "step: 153   loss: 3972.151123046875\n",
      "step: 154   loss: 3927.08056640625\n",
      "step: 155   loss: 3882.786376953125\n",
      "step: 156   loss: 3839.2548828125\n",
      "step: 157   loss: 3796.46435546875\n",
      "step: 158   loss: 3754.40087890625\n",
      "step: 159   loss: 3713.041259765625\n",
      "step: 160   loss: 3672.384521484375\n",
      "step: 161   loss: 3632.436279296875\n",
      "step: 162   loss: 3593.16162109375\n",
      "step: 163   loss: 3554.534912109375\n",
      "step: 164   loss: 3516.540771484375\n",
      "step: 165   loss: 3479.170166015625\n",
      "step: 166   loss: 3442.40283203125\n",
      "step: 167   loss: 3406.22216796875\n",
      "step: 168   loss: 3370.62744140625\n",
      "step: 169   loss: 3335.601806640625\n",
      "step: 170   loss: 3301.1298828125\n",
      "step: 171   loss: 3267.212890625\n",
      "step: 172   loss: 3233.82666015625\n",
      "step: 173   loss: 3200.96533203125\n",
      "step: 174   loss: 3168.6142578125\n",
      "step: 175   loss: 3136.7587890625\n",
      "step: 176   loss: 3105.48779296875\n",
      "step: 177   loss: 3074.722412109375\n",
      "step: 178   loss: 3044.428955078125\n",
      "step: 179   loss: 3014.59423828125\n",
      "step: 180   loss: 2985.206298828125\n",
      "step: 181   loss: 2956.263427734375\n",
      "step: 182   loss: 2927.748779296875\n",
      "step: 183   loss: 2899.658935546875\n",
      "step: 184   loss: 2871.98388671875\n",
      "step: 185   loss: 2844.71240234375\n",
      "step: 186   loss: 2817.839599609375\n",
      "step: 187   loss: 2791.361572265625\n",
      "step: 188   loss: 2765.274169921875\n",
      "step: 189   loss: 2739.5556640625\n",
      "step: 190   loss: 2714.213134765625\n",
      "step: 191   loss: 2689.228515625\n",
      "step: 192   loss: 2664.597900390625\n",
      "step: 193   loss: 2640.31982421875\n",
      "step: 194   loss: 2616.3828125\n",
      "step: 195   loss: 2592.77880859375\n",
      "step: 196   loss: 2569.50732421875\n",
      "step: 197   loss: 2546.561279296875\n",
      "step: 198   loss: 2523.934814453125\n",
      "step: 199   loss: 2501.616943359375\n",
      "step: 200   loss: 2479.611083984375\n",
      "step: 201   loss: 2457.90576171875\n",
      "step: 202   loss: 2436.491455078125\n",
      "step: 203   loss: 2415.36962890625\n",
      "step: 204   loss: 2394.532958984375\n",
      "step: 205   loss: 2373.97265625\n",
      "step: 206   loss: 2353.693603515625\n",
      "step: 207   loss: 2333.681640625\n",
      "step: 208   loss: 2313.935302734375\n",
      "step: 209   loss: 2294.45361328125\n",
      "step: 210   loss: 2275.2314453125\n",
      "step: 211   loss: 2256.251708984375\n",
      "step: 212   loss: 2237.52294921875\n",
      "step: 213   loss: 2219.03857421875\n",
      "step: 214   loss: 2200.791015625\n",
      "step: 215   loss: 2182.78369140625\n",
      "step: 216   loss: 2165.003173828125\n",
      "step: 217   loss: 2147.44580078125\n",
      "step: 218   loss: 2130.11865234375\n",
      "step: 219   loss: 2113.007568359375\n",
      "step: 220   loss: 2096.114990234375\n",
      "step: 221   loss: 2079.433349609375\n",
      "step: 222   loss: 2063.074951171875\n",
      "step: 223   loss: 2046.925537109375\n",
      "step: 224   loss: 2030.9805908203125\n",
      "step: 225   loss: 2015.2333984375\n",
      "step: 226   loss: 1999.6756591796875\n",
      "step: 227   loss: 1984.3070068359375\n",
      "step: 228   loss: 1969.123779296875\n",
      "step: 229   loss: 1954.12548828125\n",
      "step: 230   loss: 1939.315673828125\n",
      "step: 231   loss: 1924.6766357421875\n",
      "step: 232   loss: 1910.214111328125\n",
      "step: 233   loss: 1895.926513671875\n",
      "step: 234   loss: 1881.8056640625\n",
      "step: 235   loss: 1867.8531494140625\n",
      "step: 236   loss: 1854.0665283203125\n",
      "step: 237   loss: 1840.4398193359375\n",
      "step: 238   loss: 1826.97509765625\n",
      "step: 239   loss: 1813.6671142578125\n",
      "step: 240   loss: 1800.513916015625\n",
      "step: 241   loss: 1787.517333984375\n",
      "step: 242   loss: 1774.67578125\n",
      "step: 243   loss: 1761.985595703125\n",
      "step: 244   loss: 1749.4410400390625\n",
      "step: 245   loss: 1737.0380859375\n",
      "step: 246   loss: 1724.779541015625\n",
      "step: 247   loss: 1712.659423828125\n",
      "step: 248   loss: 1700.6748046875\n",
      "step: 249   loss: 1688.822021484375\n",
      "step: 250   loss: 1677.102783203125\n",
      "step: 251   loss: 1665.5150146484375\n",
      "step: 252   loss: 1654.0550537109375\n",
      "step: 253   loss: 1642.7225341796875\n",
      "step: 254   loss: 1631.51611328125\n",
      "step: 255   loss: 1620.429931640625\n",
      "step: 256   loss: 1609.4661865234375\n",
      "step: 257   loss: 1598.6209716796875\n",
      "step: 258   loss: 1587.8914794921875\n",
      "step: 259   loss: 1577.282958984375\n",
      "step: 260   loss: 1566.7867431640625\n",
      "step: 261   loss: 1556.403564453125\n",
      "step: 262   loss: 1546.1292724609375\n",
      "step: 263   loss: 1535.966552734375\n",
      "step: 264   loss: 1525.921875\n",
      "step: 265   loss: 1515.9825439453125\n",
      "step: 266   loss: 1506.14892578125\n",
      "step: 267   loss: 1496.418212890625\n",
      "step: 268   loss: 1486.7886962890625\n",
      "step: 269   loss: 1477.260986328125\n",
      "step: 270   loss: 1467.8310546875\n",
      "step: 271   loss: 1458.5006103515625\n",
      "step: 272   loss: 1449.2657470703125\n",
      "step: 273   loss: 1440.1241455078125\n",
      "step: 274   loss: 1431.0787353515625\n",
      "step: 275   loss: 1422.123291015625\n",
      "step: 276   loss: 1413.2584228515625\n",
      "step: 277   loss: 1404.4833984375\n",
      "step: 278   loss: 1395.801025390625\n",
      "step: 279   loss: 1387.204833984375\n",
      "step: 280   loss: 1378.6944580078125\n",
      "step: 281   loss: 1370.26806640625\n",
      "step: 282   loss: 1361.927001953125\n",
      "step: 283   loss: 1353.6673583984375\n",
      "step: 284   loss: 1345.49169921875\n",
      "step: 285   loss: 1337.39990234375\n",
      "step: 286   loss: 1329.38623046875\n",
      "step: 287   loss: 1321.450439453125\n",
      "step: 288   loss: 1313.593017578125\n",
      "step: 289   loss: 1305.813232421875\n",
      "step: 290   loss: 1298.1104736328125\n",
      "step: 291   loss: 1290.4793701171875\n",
      "step: 292   loss: 1282.9251708984375\n",
      "step: 293   loss: 1275.4403076171875\n",
      "step: 294   loss: 1268.03173828125\n",
      "step: 295   loss: 1260.691162109375\n",
      "step: 296   loss: 1253.4207763671875\n",
      "step: 297   loss: 1246.21826171875\n",
      "step: 298   loss: 1239.0850830078125\n",
      "step: 299   loss: 1232.019775390625\n",
      "step: 300   loss: 1225.0196533203125\n",
      "step: 301   loss: 1218.0869140625\n",
      "step: 302   loss: 1211.2176513671875\n",
      "step: 303   loss: 1204.4127197265625\n",
      "step: 304   loss: 1197.671142578125\n",
      "step: 305   loss: 1190.99267578125\n",
      "step: 306   loss: 1184.3753662109375\n",
      "step: 307   loss: 1177.8189697265625\n",
      "step: 308   loss: 1171.32421875\n",
      "step: 309   loss: 1164.887451171875\n",
      "step: 310   loss: 1158.5098876953125\n",
      "step: 311   loss: 1152.1895751953125\n",
      "step: 312   loss: 1145.9278564453125\n",
      "step: 313   loss: 1139.7232666015625\n",
      "step: 314   loss: 1133.5733642578125\n",
      "step: 315   loss: 1127.477783203125\n",
      "step: 316   loss: 1121.438232421875\n",
      "step: 317   loss: 1115.4530029296875\n",
      "step: 318   loss: 1109.521728515625\n",
      "step: 319   loss: 1103.6402587890625\n",
      "step: 320   loss: 1097.8133544921875\n",
      "step: 321   loss: 1092.038330078125\n",
      "step: 322   loss: 1086.31298828125\n",
      "step: 323   loss: 1080.63720703125\n",
      "step: 324   loss: 1075.013671875\n",
      "step: 325   loss: 1069.438232421875\n",
      "step: 326   loss: 1063.910888671875\n",
      "step: 327   loss: 1058.4315185546875\n",
      "step: 328   loss: 1052.998291015625\n",
      "step: 329   loss: 1047.614501953125\n",
      "step: 330   loss: 1042.276123046875\n",
      "step: 331   loss: 1036.9820556640625\n",
      "step: 332   loss: 1031.7340087890625\n",
      "step: 333   loss: 1026.53125\n",
      "step: 334   loss: 1021.3737182617188\n",
      "step: 335   loss: 1016.2599487304688\n",
      "step: 336   loss: 1011.1885986328125\n",
      "step: 337   loss: 1006.2249755859375\n",
      "step: 338   loss: 1001.3125\n",
      "step: 339   loss: 996.4418334960938\n",
      "step: 340   loss: 991.6111450195312\n",
      "step: 341   loss: 986.8208618164062\n",
      "step: 342   loss: 982.07080078125\n",
      "step: 343   loss: 977.3602294921875\n",
      "step: 344   loss: 972.687744140625\n",
      "step: 345   loss: 968.0535888671875\n",
      "step: 346   loss: 963.4591064453125\n",
      "step: 347   loss: 958.9014892578125\n",
      "step: 348   loss: 954.38232421875\n",
      "step: 349   loss: 949.8980712890625\n",
      "step: 350   loss: 945.45068359375\n",
      "step: 351   loss: 941.0393676757812\n",
      "step: 352   loss: 936.66357421875\n",
      "step: 353   loss: 932.3233032226562\n",
      "step: 354   loss: 928.0185546875\n",
      "step: 355   loss: 923.74755859375\n",
      "step: 356   loss: 919.5115966796875\n",
      "step: 357   loss: 915.3094482421875\n",
      "step: 358   loss: 911.139404296875\n",
      "step: 359   loss: 907.0025024414062\n",
      "step: 360   loss: 902.8998413085938\n",
      "step: 361   loss: 898.8287353515625\n",
      "step: 362   loss: 894.7896728515625\n",
      "step: 363   loss: 890.7815551757812\n",
      "step: 364   loss: 886.80712890625\n",
      "step: 365   loss: 882.8621215820312\n",
      "step: 366   loss: 878.9481201171875\n",
      "step: 367   loss: 875.0648193359375\n",
      "step: 368   loss: 871.2122802734375\n",
      "step: 369   loss: 867.38818359375\n",
      "step: 370   loss: 863.5936889648438\n",
      "step: 371   loss: 859.8289794921875\n",
      "step: 372   loss: 856.0933227539062\n",
      "step: 373   loss: 852.3855590820312\n",
      "step: 374   loss: 848.706298828125\n",
      "step: 375   loss: 845.054931640625\n",
      "step: 376   loss: 841.4312744140625\n",
      "step: 377   loss: 837.8339233398438\n",
      "step: 378   loss: 834.2645874023438\n",
      "step: 379   loss: 830.7244262695312\n",
      "step: 380   loss: 827.208740234375\n",
      "step: 381   loss: 823.7191772460938\n",
      "step: 382   loss: 820.2557373046875\n",
      "step: 383   loss: 816.818603515625\n",
      "step: 384   loss: 813.4066772460938\n",
      "step: 385   loss: 810.0208129882812\n",
      "step: 386   loss: 806.6595458984375\n",
      "step: 387   loss: 803.32275390625\n",
      "step: 388   loss: 800.0111083984375\n",
      "step: 389   loss: 796.7221069335938\n",
      "step: 390   loss: 793.4583129882812\n",
      "step: 391   loss: 790.2196655273438\n",
      "step: 392   loss: 787.003173828125\n",
      "step: 393   loss: 783.810791015625\n",
      "step: 394   loss: 780.6412963867188\n",
      "step: 395   loss: 777.4944458007812\n",
      "step: 396   loss: 774.3707885742188\n",
      "step: 397   loss: 771.2703857421875\n",
      "step: 398   loss: 768.192626953125\n",
      "step: 399   loss: 765.1357421875\n",
      "step: 400   loss: 762.1014404296875\n",
      "step: 401   loss: 759.1075439453125\n",
      "step: 402   loss: 756.1428833007812\n",
      "step: 403   loss: 753.1983032226562\n",
      "step: 404   loss: 750.2758178710938\n",
      "step: 405   loss: 747.3731079101562\n",
      "step: 406   loss: 744.4906005859375\n",
      "step: 407   loss: 741.6278076171875\n",
      "step: 408   loss: 738.7860107421875\n",
      "step: 409   loss: 735.9629516601562\n",
      "step: 410   loss: 733.160400390625\n",
      "step: 411   loss: 730.3776245117188\n",
      "step: 412   loss: 727.6129150390625\n",
      "step: 413   loss: 724.8671875\n",
      "step: 414   loss: 722.1412963867188\n",
      "step: 415   loss: 719.436279296875\n",
      "step: 416   loss: 716.7561645507812\n",
      "step: 417   loss: 714.0947875976562\n",
      "step: 418   loss: 711.450927734375\n",
      "step: 419   loss: 708.8248901367188\n",
      "step: 420   loss: 706.2166748046875\n",
      "step: 421   loss: 703.6260986328125\n",
      "step: 422   loss: 701.0526123046875\n",
      "step: 423   loss: 698.4967041015625\n",
      "step: 424   loss: 695.9577026367188\n",
      "step: 425   loss: 693.4364624023438\n",
      "step: 426   loss: 690.9306640625\n",
      "step: 427   loss: 688.4420776367188\n",
      "step: 428   loss: 685.96923828125\n",
      "step: 429   loss: 683.512939453125\n",
      "step: 430   loss: 681.0728759765625\n",
      "step: 431   loss: 678.6485595703125\n",
      "step: 432   loss: 676.2403564453125\n",
      "step: 433   loss: 673.8475952148438\n",
      "step: 434   loss: 671.470458984375\n",
      "step: 435   loss: 669.1089477539062\n",
      "step: 436   loss: 666.7628173828125\n",
      "step: 437   loss: 664.4318237304688\n",
      "step: 438   loss: 662.1156005859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 439   loss: 659.8140869140625\n",
      "step: 440   loss: 657.5277099609375\n",
      "step: 441   loss: 655.2560424804688\n",
      "step: 442   loss: 652.9986572265625\n",
      "step: 443   loss: 650.7556762695312\n",
      "step: 444   loss: 648.527587890625\n",
      "step: 445   loss: 646.3128051757812\n",
      "step: 446   loss: 644.11279296875\n",
      "step: 447   loss: 641.9264526367188\n",
      "step: 448   loss: 639.7538452148438\n",
      "step: 449   loss: 637.5944213867188\n",
      "step: 450   loss: 635.4494018554688\n",
      "step: 451   loss: 633.3182373046875\n",
      "step: 452   loss: 631.1998291015625\n",
      "step: 453   loss: 629.0948486328125\n",
      "step: 454   loss: 627.0020141601562\n",
      "step: 455   loss: 624.9228515625\n",
      "step: 456   loss: 622.856201171875\n",
      "step: 457   loss: 620.802978515625\n",
      "step: 458   loss: 618.7622680664062\n",
      "step: 459   loss: 616.73388671875\n",
      "step: 460   loss: 614.7181396484375\n",
      "step: 461   loss: 612.7149047851562\n",
      "step: 462   loss: 610.723388671875\n",
      "step: 463   loss: 608.7442016601562\n",
      "step: 464   loss: 606.7772827148438\n",
      "step: 465   loss: 604.8221435546875\n",
      "step: 466   loss: 602.8787841796875\n",
      "step: 467   loss: 600.9480590820312\n",
      "step: 468   loss: 599.0281372070312\n",
      "step: 469   loss: 597.120361328125\n",
      "step: 470   loss: 595.2239379882812\n",
      "step: 471   loss: 593.3394775390625\n",
      "step: 472   loss: 591.4652099609375\n",
      "step: 473   loss: 589.6025390625\n",
      "step: 474   loss: 587.7509765625\n",
      "step: 475   loss: 585.9100341796875\n",
      "step: 476   loss: 584.0808715820312\n",
      "step: 477   loss: 582.2626953125\n",
      "step: 478   loss: 580.4545288085938\n",
      "step: 479   loss: 578.657470703125\n",
      "step: 480   loss: 576.8704833984375\n",
      "step: 481   loss: 575.0946044921875\n",
      "step: 482   loss: 573.3284912109375\n",
      "step: 483   loss: 571.57373046875\n",
      "step: 484   loss: 569.8289794921875\n",
      "step: 485   loss: 568.0942993164062\n",
      "step: 486   loss: 566.3692016601562\n",
      "step: 487   loss: 564.6544189453125\n",
      "step: 488   loss: 562.9494018554688\n",
      "step: 489   loss: 561.25439453125\n",
      "step: 490   loss: 559.5696411132812\n",
      "step: 491   loss: 557.8941040039062\n",
      "step: 492   loss: 556.2286987304688\n",
      "step: 493   loss: 554.5723266601562\n",
      "step: 494   loss: 552.9255981445312\n",
      "step: 495   loss: 551.2883911132812\n",
      "step: 496   loss: 549.6607666015625\n",
      "step: 497   loss: 548.041748046875\n",
      "step: 498   loss: 546.4334106445312\n",
      "step: 499   loss: 544.8330688476562\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "x = torch.randn((N, D_in)).to(device)\n",
    "y = torch.randn((N, D_out)).to(device)\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "w1 = torch.rand((D_in, H)).to(device)\n",
    "w2 = torch.rand((H, D_out)).to(device)\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(500):\n",
    "  # 向前傳遞: 計算y_pred\n",
    "  #y_pred = torch.matmul(torch.relu(torch.matmul(x, w1)), w2)\n",
    "  h = torch.matmul(x, w1)   \n",
    "  h_relu = torch.relu(h)\n",
    "  y_pred = torch.matmul(h_relu, w2)  \n",
    "    \n",
    "  # 計算loss\n",
    "  loss = torch.square(y_pred - y).sum()\n",
    "  print('step:', t,'  loss:' , loss.item())\n",
    "\n",
    "  # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "  y_pred_grad = 2. * (y_pred - y) \n",
    "  w2_grad = h_relu.T.matmul(y_pred_grad) \n",
    "  h_grad = y_pred_grad.matmul(w2.T) * (h > 0.)\n",
    "  w1_grad = x.T.matmul(h_grad)\n",
    "    \n",
    "  # 參數更新\n",
    "  w1.data = w1.data - learning_rate * w1_grad\n",
    "  w2.data = w2.data - learning_rate * w2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9XiShaU16s3"
   },
   "source": [
    "### 使用Pytorch的Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VP1YW7516s4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlj3NwsP16s6",
    "outputId": "0463fd34-3edf-4516-9d36-c1143463790d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34531528.0\n",
      "1 37563884.0\n",
      "2 45605692.0\n",
      "3 47743296.0\n",
      "4 37549408.0\n",
      "5 19983466.0\n",
      "6 8218459.0\n",
      "7 3358081.75\n",
      "8 1769839.75\n",
      "9 1200070.875\n",
      "10 933849.8125\n",
      "11 768861.875\n",
      "12 647828.125\n",
      "13 552127.8125\n",
      "14 474321.3125\n",
      "15 409854.5\n",
      "16 355987.53125\n",
      "17 310554.46875\n",
      "18 271993.3125\n",
      "19 239127.03125\n",
      "20 210943.40625\n",
      "21 186672.421875\n",
      "22 165673.984375\n",
      "23 147426.53125\n",
      "24 131516.640625\n",
      "25 117607.96875\n",
      "26 105414.3125\n",
      "27 94697.796875\n",
      "28 85241.1640625\n",
      "29 76882.578125\n",
      "30 69470.8125\n",
      "31 62886.90625\n",
      "32 57027.1640625\n",
      "33 51796.1875\n",
      "34 47115.85546875\n",
      "35 42919.08203125\n",
      "36 39149.59375\n",
      "37 35757.9609375\n",
      "38 32700.904296875\n",
      "39 29940.25390625\n",
      "40 27445.466796875\n",
      "41 25186.919921875\n",
      "42 23138.40234375\n",
      "43 21277.962890625\n",
      "44 19586.13671875\n",
      "45 18047.291015625\n",
      "46 16644.80859375\n",
      "47 15364.494140625\n",
      "48 14194.1484375\n",
      "49 13123.5673828125\n",
      "50 12142.908203125\n",
      "51 11244.23046875\n",
      "52 10419.24609375\n",
      "53 9661.384765625\n",
      "54 8964.6005859375\n",
      "55 8323.494140625\n",
      "56 7732.97802734375\n",
      "57 7188.859375\n",
      "58 6686.65673828125\n",
      "59 6223.0908203125\n",
      "60 5794.66650390625\n",
      "61 5398.6962890625\n",
      "62 5032.1875\n",
      "63 4692.9111328125\n",
      "64 4378.51953125\n",
      "65 4087.063232421875\n",
      "66 3816.718994140625\n",
      "67 3565.863525390625\n",
      "68 3332.81982421875\n",
      "69 3116.280029296875\n",
      "70 2914.98779296875\n",
      "71 2727.807861328125\n",
      "72 2553.439208984375\n",
      "73 2391.10986328125\n",
      "74 2239.864501953125\n",
      "75 2098.887451171875\n",
      "76 1967.42822265625\n",
      "77 1844.76513671875\n",
      "78 1730.3004150390625\n",
      "79 1623.4085693359375\n",
      "80 1523.5950927734375\n",
      "81 1430.4342041015625\n",
      "82 1343.368896484375\n",
      "83 1261.9239501953125\n",
      "84 1185.722412109375\n",
      "85 1114.44970703125\n",
      "86 1047.693115234375\n",
      "87 985.1826782226562\n",
      "88 926.614990234375\n",
      "89 871.73193359375\n",
      "90 820.29541015625\n",
      "91 772.0489501953125\n",
      "92 726.808349609375\n",
      "93 684.3780517578125\n",
      "94 644.5438842773438\n",
      "95 607.1448364257812\n",
      "96 572.0328979492188\n",
      "97 539.0540161132812\n",
      "98 508.07354736328125\n",
      "99 478.96563720703125\n",
      "100 451.604736328125\n",
      "101 425.88018798828125\n",
      "102 401.6925048828125\n",
      "103 378.9407958984375\n",
      "104 357.54229736328125\n",
      "105 337.39892578125\n",
      "106 318.4451904296875\n",
      "107 300.6091613769531\n",
      "108 283.8120422363281\n",
      "109 267.989501953125\n",
      "110 253.08807373046875\n",
      "111 239.05471801757812\n",
      "112 225.82858276367188\n",
      "113 213.36328125\n",
      "114 201.61358642578125\n",
      "115 190.53688049316406\n",
      "116 180.0966796875\n",
      "117 170.24562072753906\n",
      "118 160.9562530517578\n",
      "119 152.1905975341797\n",
      "120 143.9220733642578\n",
      "121 136.11849975585938\n",
      "122 128.75534057617188\n",
      "123 121.80561065673828\n",
      "124 115.2406234741211\n",
      "125 109.04695129394531\n",
      "126 103.19266510009766\n",
      "127 97.6634292602539\n",
      "128 92.442138671875\n",
      "129 87.50855255126953\n",
      "130 82.84756469726562\n",
      "131 78.44112396240234\n",
      "132 74.27690887451172\n",
      "133 70.34107971191406\n",
      "134 66.6207275390625\n",
      "135 63.1021728515625\n",
      "136 59.77553176879883\n",
      "137 56.62983703613281\n",
      "138 53.654197692871094\n",
      "139 50.840911865234375\n",
      "140 48.17878341674805\n",
      "141 45.65916442871094\n",
      "142 43.27562713623047\n",
      "143 41.019065856933594\n",
      "144 38.88439178466797\n",
      "145 36.86332702636719\n",
      "146 34.950504302978516\n",
      "147 33.13881301879883\n",
      "148 31.424692153930664\n",
      "149 29.8005313873291\n",
      "150 28.26337242126465\n",
      "151 26.806589126586914\n",
      "152 25.426860809326172\n",
      "153 24.12084197998047\n",
      "154 22.883005142211914\n",
      "155 21.709270477294922\n",
      "156 20.598899841308594\n",
      "157 19.545543670654297\n",
      "158 18.54767608642578\n",
      "159 17.601667404174805\n",
      "160 16.705080032348633\n",
      "161 15.85473346710205\n",
      "162 15.049440383911133\n",
      "163 14.285604476928711\n",
      "164 13.561037063598633\n",
      "165 12.874383926391602\n",
      "166 12.223061561584473\n",
      "167 11.605727195739746\n",
      "168 11.020174026489258\n",
      "169 10.464323043823242\n",
      "170 9.937551498413086\n",
      "171 9.437312126159668\n",
      "172 8.963107109069824\n",
      "173 8.513189315795898\n",
      "174 8.086377143859863\n",
      "175 7.681229591369629\n",
      "176 7.296681880950928\n",
      "177 6.9316487312316895\n",
      "178 6.585463523864746\n",
      "179 6.256705284118652\n",
      "180 5.944457530975342\n",
      "181 5.64851713180542\n",
      "182 5.367351055145264\n",
      "183 5.100335121154785\n",
      "184 4.846948146820068\n",
      "185 4.6063761711120605\n",
      "186 4.3778886795043945\n",
      "187 4.161011219024658\n",
      "188 3.9550633430480957\n",
      "189 3.759467601776123\n",
      "190 3.573521852493286\n",
      "191 3.3970515727996826\n",
      "192 3.2293033599853516\n",
      "193 3.070058584213257\n",
      "194 2.9187960624694824\n",
      "195 2.774954080581665\n",
      "196 2.638568639755249\n",
      "197 2.508732318878174\n",
      "198 2.385563611984253\n",
      "199 2.268322467803955\n",
      "200 2.1572182178497314\n",
      "201 2.0513763427734375\n",
      "202 1.9509085416793823\n",
      "203 1.8554041385650635\n",
      "204 1.7646210193634033\n",
      "205 1.6782487630844116\n",
      "206 1.5962097644805908\n",
      "207 1.5182842016220093\n",
      "208 1.4442311525344849\n",
      "209 1.373866081237793\n",
      "210 1.3069300651550293\n",
      "211 1.2432373762130737\n",
      "212 1.1827532052993774\n",
      "213 1.1252074241638184\n",
      "214 1.0705537796020508\n",
      "215 1.0185778141021729\n",
      "216 0.9689918160438538\n",
      "217 0.9220355749130249\n",
      "218 0.8773123025894165\n",
      "219 0.8347629904747009\n",
      "220 0.7943807244300842\n",
      "221 0.7558294534683228\n",
      "222 0.7192960381507874\n",
      "223 0.6844810247421265\n",
      "224 0.6514226198196411\n",
      "225 0.6198399662971497\n",
      "226 0.5899635553359985\n",
      "227 0.5615289807319641\n",
      "228 0.5343899726867676\n",
      "229 0.5086695551872253\n",
      "230 0.48417264223098755\n",
      "231 0.46081018447875977\n",
      "232 0.43860161304473877\n",
      "233 0.4175123870372772\n",
      "234 0.3974224925041199\n",
      "235 0.37832826375961304\n",
      "236 0.3602134585380554\n",
      "237 0.34286922216415405\n",
      "238 0.32640907168388367\n",
      "239 0.310777485370636\n",
      "240 0.2958112359046936\n",
      "241 0.28161895275115967\n",
      "242 0.2681390345096588\n",
      "243 0.255314439535141\n",
      "244 0.243042454123497\n",
      "245 0.23140761256217957\n",
      "246 0.22036060690879822\n",
      "247 0.2098432034254074\n",
      "248 0.19978532195091248\n",
      "249 0.1902402639389038\n",
      "250 0.1811450719833374\n",
      "251 0.17245498299598694\n",
      "252 0.16428345441818237\n",
      "253 0.15645775198936462\n",
      "254 0.1489693820476532\n",
      "255 0.14188022911548615\n",
      "256 0.13510072231292725\n",
      "257 0.12867040932178497\n",
      "258 0.12255969643592834\n",
      "259 0.11670143157243729\n",
      "260 0.11116176098585129\n",
      "261 0.1058778464794159\n",
      "262 0.10082710534334183\n",
      "263 0.09605121612548828\n",
      "264 0.0914769247174263\n",
      "265 0.08711458742618561\n",
      "266 0.08299009501934052\n",
      "267 0.07906235754489899\n",
      "268 0.0752904862165451\n",
      "269 0.07173074036836624\n",
      "270 0.06830476969480515\n",
      "271 0.06506508588790894\n",
      "272 0.06199737638235092\n",
      "273 0.05905076116323471\n",
      "274 0.05625084042549133\n",
      "275 0.05359476059675217\n",
      "276 0.05107182636857033\n",
      "277 0.04864552244544029\n",
      "278 0.04636111110448837\n",
      "279 0.044166386127471924\n",
      "280 0.04208727926015854\n",
      "281 0.04009511321783066\n",
      "282 0.038198087364435196\n",
      "283 0.03639495000243187\n",
      "284 0.034691013395786285\n",
      "285 0.03307061269879341\n",
      "286 0.031503528356552124\n",
      "287 0.030018676072359085\n",
      "288 0.028602654114365578\n",
      "289 0.02728031575679779\n",
      "290 0.026003355160355568\n",
      "291 0.024776775389909744\n",
      "292 0.02362806722521782\n",
      "293 0.022502612322568893\n",
      "294 0.021442251279950142\n",
      "295 0.020450830459594727\n",
      "296 0.01949785277247429\n",
      "297 0.018590154126286507\n",
      "298 0.017728082835674286\n",
      "299 0.016894284635782242\n",
      "300 0.01611153408885002\n",
      "301 0.015362607315182686\n",
      "302 0.01464769709855318\n",
      "303 0.013969779945909977\n",
      "304 0.013311109505593777\n",
      "305 0.012696958146989346\n",
      "306 0.012112639844417572\n",
      "307 0.01155757810920477\n",
      "308 0.011021625250577927\n",
      "309 0.010512067005038261\n",
      "310 0.010034576058387756\n",
      "311 0.009574752300977707\n",
      "312 0.009136170148849487\n",
      "313 0.008717862889170647\n",
      "314 0.008319663815200329\n",
      "315 0.007941961288452148\n",
      "316 0.00758288474753499\n",
      "317 0.007241733372211456\n",
      "318 0.006913291290402412\n",
      "319 0.00660156924277544\n",
      "320 0.006311459932476282\n",
      "321 0.006026844494044781\n",
      "322 0.005757137201726437\n",
      "323 0.005506204906851053\n",
      "324 0.005260736681520939\n",
      "325 0.005028983578085899\n",
      "326 0.004805500619113445\n",
      "327 0.004594183061271906\n",
      "328 0.004392342641949654\n",
      "329 0.004205064848065376\n",
      "330 0.004023012239485979\n",
      "331 0.0038487331476062536\n",
      "332 0.0036871563643217087\n",
      "333 0.003526475979015231\n",
      "334 0.0033784927800297737\n",
      "335 0.0032341040205210447\n",
      "336 0.003097628243267536\n",
      "337 0.002969191875308752\n",
      "338 0.0028437539003789425\n",
      "339 0.0027249015402048826\n",
      "340 0.0026151505298912525\n",
      "341 0.0025076980236917734\n",
      "342 0.002407591324299574\n",
      "343 0.002308464143425226\n",
      "344 0.002214257139712572\n",
      "345 0.0021253449376672506\n",
      "346 0.0020400448702275753\n",
      "347 0.0019583404064178467\n",
      "348 0.001883871154859662\n",
      "349 0.00180826336145401\n",
      "350 0.001740465173497796\n",
      "351 0.0016740665305405855\n",
      "352 0.0016089074779301882\n",
      "353 0.001545357285067439\n",
      "354 0.0014880583621561527\n",
      "355 0.001432326389476657\n",
      "356 0.0013802156317979097\n",
      "357 0.0013281034771353006\n",
      "358 0.0012798538664355874\n",
      "359 0.001235466101206839\n",
      "360 0.0011905147694051266\n",
      "361 0.0011477130465209484\n",
      "362 0.001105836359784007\n",
      "363 0.0010658899554982781\n",
      "364 0.0010287471814081073\n",
      "365 0.0009934223489835858\n",
      "366 0.000957462762016803\n",
      "367 0.000924360123462975\n",
      "368 0.00089359370758757\n",
      "369 0.0008636488928459585\n",
      "370 0.0008341750362887979\n",
      "371 0.000806263240519911\n",
      "372 0.000779984169639647\n",
      "373 0.0007536238990724087\n",
      "374 0.000728618586435914\n",
      "375 0.0007049505948089063\n",
      "376 0.0006820313283242285\n",
      "377 0.0006610152777284384\n",
      "378 0.0006388960755430162\n",
      "379 0.0006197842885740101\n",
      "380 0.0006002169684506953\n",
      "381 0.0005804136744700372\n",
      "382 0.0005627244245260954\n",
      "383 0.000545827264431864\n",
      "384 0.0005291822599247098\n",
      "385 0.0005142483860254288\n",
      "386 0.0004979419754818082\n",
      "387 0.0004832785052713007\n",
      "388 0.00046916879364289343\n",
      "389 0.00045454857172444463\n",
      "390 0.00044278387213125825\n",
      "391 0.0004292518424335867\n",
      "392 0.00041763417539186776\n",
      "393 0.0004059297498315573\n",
      "394 0.00039374714833684266\n",
      "395 0.0003831168869510293\n",
      "396 0.00037243860424496233\n",
      "397 0.00036203867057338357\n",
      "398 0.0003529419482219964\n",
      "399 0.00034281055559404194\n",
      "400 0.0003336283261887729\n",
      "401 0.000324822380207479\n",
      "402 0.00031662522815167904\n",
      "403 0.00030820679967291653\n",
      "404 0.0003004999889526516\n",
      "405 0.00029278500005602837\n",
      "406 0.0002846720162779093\n",
      "407 0.0002771156432572752\n",
      "408 0.00027060709544457495\n",
      "409 0.0002638848382048309\n",
      "410 0.0002569505595602095\n",
      "411 0.00025035045109689236\n",
      "412 0.0002443745033815503\n",
      "413 0.00023885763948783278\n",
      "414 0.000233028331422247\n",
      "415 0.00022688508033752441\n",
      "416 0.00022209568123798817\n",
      "417 0.00021609995746985078\n",
      "418 0.0002114980306942016\n",
      "419 0.00020721254986710846\n",
      "420 0.0002023064880631864\n",
      "421 0.00019766732293646783\n",
      "422 0.0001936448534252122\n",
      "423 0.00018955471750814468\n",
      "424 0.00018501374870538712\n",
      "425 0.00018066017946694046\n",
      "426 0.000176486762939021\n",
      "427 0.000172502375789918\n",
      "428 0.0001689062628429383\n",
      "429 0.00016550574218854308\n",
      "430 0.00016185348795261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 0.00015894130046945065\n",
      "432 0.00015560245083179325\n",
      "433 0.00015244056703522801\n",
      "434 0.00014918063243385404\n",
      "435 0.00014623792958445847\n",
      "436 0.00014322339848149568\n",
      "437 0.0001401646004524082\n",
      "438 0.0001371508842566982\n",
      "439 0.00013460413902066648\n",
      "440 0.00013200324610807002\n",
      "441 0.00012920792505610734\n",
      "442 0.0001268077758140862\n",
      "443 0.0001243362348759547\n",
      "444 0.00012227214756421745\n",
      "445 0.00011963630095124245\n",
      "446 0.00011724793148459867\n",
      "447 0.00011501400149427354\n",
      "448 0.00011302570783300325\n",
      "449 0.00011090389307355508\n",
      "450 0.00010875487350858748\n",
      "451 0.00010680056584533304\n",
      "452 0.00010480251512490213\n",
      "453 0.00010332858073525131\n",
      "454 0.00010135072079719976\n",
      "455 9.952658729162067e-05\n",
      "456 9.77267773123458e-05\n",
      "457 9.615623275749385e-05\n",
      "458 9.459262946620584e-05\n",
      "459 9.324532584287226e-05\n",
      "460 9.124297503149137e-05\n",
      "461 8.984180021798238e-05\n",
      "462 8.840537338983268e-05\n",
      "463 8.65921683725901e-05\n",
      "464 8.517434616805986e-05\n",
      "465 8.388777496293187e-05\n",
      "466 8.267535304185003e-05\n",
      "467 8.130612695822492e-05\n",
      "468 8.008595614228398e-05\n",
      "469 7.88512043072842e-05\n",
      "470 7.732478843536228e-05\n",
      "471 7.603559788549319e-05\n",
      "472 7.497188926208764e-05\n",
      "473 7.382663170574233e-05\n",
      "474 7.28075101505965e-05\n",
      "475 7.146069401642308e-05\n",
      "476 7.041034405119717e-05\n",
      "477 6.908102659508586e-05\n",
      "478 6.819383997935802e-05\n",
      "479 6.714554183417931e-05\n",
      "480 6.63465034449473e-05\n",
      "481 6.541830225614831e-05\n",
      "482 6.438646232709289e-05\n",
      "483 6.350671901600435e-05\n",
      "484 6.245985423447564e-05\n",
      "485 6.16528486716561e-05\n",
      "486 6.073986514820717e-05\n",
      "487 5.980260903015733e-05\n",
      "488 5.939040056546219e-05\n",
      "489 5.856867937836796e-05\n",
      "490 5.7517492678016424e-05\n",
      "491 5.6789853260852396e-05\n",
      "492 5.588223211816512e-05\n",
      "493 5.5120653996709734e-05\n",
      "494 5.4308580729411915e-05\n",
      "495 5.381513255997561e-05\n",
      "496 5.3486903198063374e-05\n",
      "497 5.2565424994099885e-05\n",
      "498 5.177072307560593e-05\n",
      "499 5.095719461678527e-05\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "x = torch.randn((N, D_in)).to(device)\n",
    "y = torch.randn((N, D_out)).to(device)\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "w1 = torch.randn((D_in, H), requires_grad=True).to(device)\n",
    "w2 = torch.randn((H, D_out), requires_grad=True).to(device)\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(500):\n",
    "  # 向前傳遞: 計算y_pred\n",
    "  h = torch.matmul(x, w1)\n",
    "  h_relu = torch.relu(h)\n",
    "  y_pred = torch.matmul(h_relu, w2)\n",
    "  \n",
    "  # 計算loss\n",
    "  loss = torch.square(y_pred - y).sum() \n",
    "  print('step:',t,' loss' ,loss.item())\n",
    "\n",
    "  # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "  loss.backward()\n",
    "\n",
    "  # 參數更新: 這裡再更新參數時，我們不希望更新參數的計算也被紀錄微分相關的資訊，因此使用torch.no_grad()\n",
    "  with torch.no_grad():\n",
    "    # 更新參數W1 W2\n",
    "    w1.data = w1\n",
    "\n",
    "    # 將紀錄的gradient清空(因為已經更新參數)\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znJFnEdr16s9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "微分與倒傳遞作業.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
